{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Author : \n",
    "Nishi Cestero\n",
    "\n",
    "This notebook details data acquisition from Reddit using the Python Pushshift API wrapper. This method allows us to more easily retrieve historical posts without running into post-retrieval limits of the original Python API. For reference, I've kept code of using the original Python Reddit API at the bottom.\n",
    "\n",
    "As of 4/19/2020: scraped subreddit data from January 2016 to April, 19, 2020. Start date corresponds to the year r/BPD began using \"flairs\" in their posts. Set the same starting date for all other subreddits for consistency. \n",
    "\n",
    "Subreddits scraped:\n",
    "- General:\n",
    "    - r/Advice\n",
    "    - r/needadvice\n",
    "    - r/Vent\n",
    "    - r/venting\n",
    "    - r/Rant\n",
    "- Mental Health:\n",
    "    - r/BPD\n",
    "    - r/CPTSD\n",
    "    - r/Anxiety\n",
    "    - r/depressionhelp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Pushshift API wrapper method\n",
    "\n",
    "PSAW documentation:\n",
    "https://github.com/dmarx/psaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from psaw import PushshiftAPI\n",
    "\n",
    "api = PushshiftAPI()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of using API "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "    \n",
    "start_epoch=int(dt.datetime(2020,3,1).timestamp())\n",
    "end_epoch=int(dt.datetime(2020,4,1).timestamp())\n",
    "\n",
    "time_now = int(dt.datetime.utcnow().timestamp())\n",
    "results = list(api.search_submissions(after=start_epoch,\n",
    "                                  before=end_epoch,\n",
    "                        subreddit='BPD',\n",
    "                        filter=['id','created_utc' ,'author', 'title','selftext', 'subreddit','num_comments', 'ups',\n",
    "                               'downs','score','url','link_flair_text' ],\n",
    "                                  limit=10000\n",
    "                        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of a submission object's dictionary returned by API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'author': 'ccholericc',\n",
       " 'created_utc': 1585712740,\n",
       " 'id': 'fsrzxr',\n",
       " 'link_flair_text': 'Positivity',\n",
       " 'num_comments': 9,\n",
       " 'score': 3,\n",
       " 'selftext': 'After everything that has been going on around the world, this is one of the only places I feel safe enough to talk about my feelings. Everyone here really understands what I’m going through and feeling and nobody ever judges. I’m not trying to romanticize BPD, but you are some of the most compassionate and kind people I’ve ever met. I hope everyone is coping well with the chaos of today, if not at least know we’re all going through it together',\n",
       " 'subreddit': 'BPD',\n",
       " 'title': 'I feel safe in this sub',\n",
       " 'url': 'https://www.reddit.com/r/BPD/comments/fsrzxr/i_feel_safe_in_this_sub/',\n",
       " 'created': 1585727140.0}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0].d_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## crawl_reddit(...)\n",
    "\n",
    "Retrieve all posts in a subreddit from given start year/month to present time. Queries API month-per-month to avoid results limitation.\n",
    "\n",
    "By default only returns flaired posts. Specify to return all posts regardless of having flair. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt\n",
    "\n",
    "#crawl all submissions from a subreddit from start time to current in monthly increments\n",
    "def crawl_reddit(start_year, start_month, subreddit_name, filter_nonflaired=True):\n",
    "    \n",
    "    df = pd.DataFrame()\n",
    "    time_now = int(dt.datetime.utcnow().timestamp())    \n",
    "    start_epoch = int(dt.datetime(start_year, start_month, 1).timestamp())\n",
    "\n",
    "    for year in range(start_year,2021):\n",
    "        for month in range(start_month, 13):\n",
    "            start_epoch = int(dt.datetime(year, month, 1).timestamp())\n",
    "            \n",
    "            if(month==12):\n",
    "                end_epoch = int(dt.datetime(year+1, 1, 1).timestamp())\n",
    "            else:\n",
    "                end_epoch = int(dt.datetime(year, month+1, 1).timestamp())\n",
    "            \n",
    "            if( start_epoch > time_now):\n",
    "                break\n",
    "\n",
    "\n",
    "            results = list(api.search_submissions(\n",
    "                        after=start_epoch, before=end_epoch,\n",
    "                        subreddit=subreddit_name,\n",
    "                        filter=['id','created_utc' ,'author', 'title','selftext', 'subreddit',\n",
    "                                'num_comments','score','url','link_flair_text' ],\n",
    "                        limit=100000\n",
    "                    ))\n",
    "\n",
    "            if (filter_nonflaired):\n",
    "                #remove objects missing the link_flair_text attribute\n",
    "                results= [submission for submission in results if submission.d_.get('link_flair_text','NA') != 'NA']\n",
    "            else:\n",
    "                #make sure at least one submission carries 'link_flair_text' in .d_\n",
    "                results[0].d_['link_flair_text'] = results[0].d_.get('link_flair_text',np.nan)\n",
    "                results = [submission.d_ for submission in results]\n",
    "            \n",
    "            print(year, month)\n",
    "            print(len(results))\n",
    "            \n",
    "            if(len(results)>0):\n",
    "                df = df.append(results)\n",
    "                \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mental Health Subreddits with Intention Tags\n",
    "\n",
    "Subreddits (Member Count) : [ List of Relevant Flairs]\n",
    "- r/BPD (88.9k) : ['Venting', 'Input', 'Seeking Support',  + older flairs]\n",
    "- r/CPTSD (65.3k): ['CPTSD Vent/ Rant' , 'Request: Emotional Support']\n",
    "- r/depression_help (51.2k): [ 'REQUESTING SUPPORT', 'REQUESTING ADVICE' , 'RANT' ]\n",
    "- r/Anxiety (349k) : ['Advice Needed', 'Needs A Hug/Support', 'Venting']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 1\n",
      "498\n",
      "2016 2\n",
      "498\n",
      "2016 3\n",
      "490\n",
      "2016 4\n",
      "450\n",
      "2016 5\n",
      "508\n",
      "2016 6\n",
      "486\n",
      "2016 7\n",
      "531\n",
      "2016 8\n",
      "542\n",
      "2016 9\n",
      "323\n",
      "2016 10\n",
      "122\n",
      "2016 11\n",
      "80\n",
      "2016 12\n",
      "70\n",
      "2017 1\n",
      "71\n",
      "2017 2\n",
      "43\n",
      "2017 3\n",
      "44\n",
      "2017 4\n",
      "93\n",
      "2017 5\n",
      "173\n",
      "2017 6\n",
      "173\n",
      "2017 7\n",
      "198\n",
      "2017 8\n",
      "204\n",
      "2017 9\n",
      "216\n",
      "2017 10\n",
      "251\n",
      "2017 11\n",
      "264\n",
      "2017 12\n",
      "324\n",
      "2018 1\n",
      "374\n",
      "2018 2\n",
      "378\n",
      "2018 3\n",
      "356\n",
      "2018 4\n",
      "409\n",
      "2018 5\n",
      "458\n",
      "2018 6\n",
      "500\n",
      "2018 7\n",
      "635\n",
      "2018 8\n",
      "634\n",
      "2018 9\n",
      "603\n",
      "2018 10\n",
      "657\n",
      "2018 11\n",
      "715\n",
      "2018 12\n",
      "771\n",
      "2019 1\n",
      "1000\n",
      "2019 2\n",
      "1835\n",
      "2019 3\n",
      "2218\n",
      "2019 4\n",
      "1978\n",
      "2019 5\n",
      "2069\n",
      "2019 6\n",
      "2058\n",
      "2019 7\n",
      "2067\n",
      "2019 8\n",
      "2245\n",
      "2019 9\n",
      "2209\n",
      "2019 10\n",
      "2513\n",
      "2019 11\n",
      "2413\n",
      "2019 12\n",
      "2754\n",
      "2020 1\n",
      "3540\n",
      "2020 2\n",
      "3457\n",
      "2020 3\n",
      "3997\n",
      "2020 4\n",
      "2579\n"
     ]
    }
   ],
   "source": [
    "BPD_df = crawl_reddit(2016,1,'BPD')\n",
    "BPD_df.to_csv('BPD_01_01_2016_04_19_2020')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 1\n",
      "0\n",
      "2016 2\n",
      "0\n",
      "2016 3\n",
      "0\n",
      "2016 4\n",
      "0\n",
      "2016 5\n",
      "0\n",
      "2016 6\n",
      "0\n",
      "2016 7\n",
      "0\n",
      "2016 8\n",
      "0\n",
      "2016 9\n",
      "0\n",
      "2016 10\n",
      "0\n",
      "2016 11\n",
      "0\n",
      "2016 12\n",
      "0\n",
      "2017 1\n",
      "0\n",
      "2017 2\n",
      "0\n",
      "2017 3\n",
      "0\n",
      "2017 4\n",
      "0\n",
      "2017 5\n",
      "0\n",
      "2017 6\n",
      "0\n",
      "2017 7\n",
      "0\n",
      "2017 8\n",
      "0\n",
      "2017 9\n",
      "0\n",
      "2017 10\n",
      "0\n",
      "2017 11\n",
      "0\n",
      "2017 12\n",
      "0\n",
      "2018 1\n",
      "0\n",
      "2018 2\n",
      "0\n",
      "2018 3\n",
      "0\n",
      "2018 4\n",
      "0\n",
      "2018 5\n",
      "1\n",
      "2018 6\n",
      "0\n",
      "2018 7\n",
      "1\n",
      "2018 8\n",
      "0\n",
      "2018 9\n",
      "0\n",
      "2018 10\n",
      "0\n",
      "2018 11\n",
      "0\n",
      "2018 12\n",
      "0\n",
      "2019 1\n",
      "0\n",
      "2019 2\n",
      "48\n",
      "2019 3\n",
      "443\n",
      "2019 4\n",
      "533\n",
      "2019 5\n",
      "642\n",
      "2019 6\n",
      "628\n",
      "2019 7\n",
      "810\n",
      "2019 8\n",
      "847\n",
      "2019 9\n",
      "987\n",
      "2019 10\n",
      "1083\n",
      "2019 11\n",
      "1056\n",
      "2019 12\n",
      "1357\n",
      "2020 1\n",
      "1361\n",
      "2020 2\n",
      "1426\n",
      "2020 3\n",
      "1420\n",
      "2020 4\n",
      "924\n"
     ]
    }
   ],
   "source": [
    "CPTSD_df = crawl_reddit(2016,1,'CPTSD')\n",
    "CPTSD_df.to_csv('CPTSD_01_01_2016_04_19_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 1\n",
      "0\n",
      "2016 2\n",
      "0\n",
      "2016 3\n",
      "0\n",
      "2016 4\n",
      "0\n",
      "2016 5\n",
      "0\n",
      "2016 6\n",
      "0\n",
      "2016 7\n",
      "0\n",
      "2016 8\n",
      "0\n",
      "2016 9\n",
      "0\n",
      "2016 10\n",
      "0\n",
      "2016 11\n",
      "0\n",
      "2016 12\n",
      "0\n",
      "2017 1\n",
      "0\n",
      "2017 2\n",
      "0\n",
      "2017 3\n",
      "0\n",
      "2017 4\n",
      "0\n",
      "2017 5\n",
      "0\n",
      "2017 6\n",
      "0\n",
      "2017 7\n",
      "0\n",
      "2017 8\n",
      "0\n",
      "2017 9\n",
      "0\n",
      "2017 10\n",
      "0\n",
      "2017 11\n",
      "0\n",
      "2017 12\n",
      "0\n",
      "2018 1\n",
      "0\n",
      "2018 2\n",
      "0\n",
      "2018 3\n",
      "0\n",
      "2018 4\n",
      "0\n",
      "2018 5\n",
      "0\n",
      "2018 6\n",
      "1\n",
      "2018 7\n",
      "0\n",
      "2018 8\n",
      "0\n",
      "2018 9\n",
      "0\n",
      "2018 10\n",
      "0\n",
      "2018 11\n",
      "0\n",
      "2018 12\n",
      "0\n",
      "2019 1\n",
      "333\n",
      "2019 2\n",
      "598\n",
      "2019 3\n",
      "1038\n",
      "2019 4\n",
      "1047\n",
      "2019 5\n",
      "947\n",
      "2019 6\n",
      "769\n",
      "2019 7\n",
      "336\n",
      "2019 8\n",
      "0\n",
      "2019 9\n",
      "0\n",
      "2019 10\n",
      "548\n",
      "2019 11\n",
      "771\n",
      "2019 12\n",
      "873\n",
      "2020 1\n",
      "980\n",
      "2020 2\n",
      "1102\n",
      "2020 3\n",
      "1063\n",
      "2020 4\n",
      "817\n"
     ]
    }
   ],
   "source": [
    "depression = crawl_reddit(2016,1,'depression_help')\n",
    "depression.to_csv('depression_help_04_19_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 1\n",
      "802\n",
      "2016 2\n",
      "801\n",
      "2016 3\n",
      "742\n",
      "2016 4\n",
      "711\n",
      "2016 5\n",
      "802\n",
      "2016 6\n",
      "753\n",
      "2016 7\n",
      "849\n",
      "2016 8\n",
      "984\n",
      "2016 9\n",
      "944\n",
      "2016 10\n",
      "1025\n",
      "2016 11\n",
      "919\n",
      "2016 12\n",
      "917\n",
      "2017 1\n",
      "1102\n",
      "2017 2\n",
      "944\n",
      "2017 3\n",
      "1113\n",
      "2017 4\n",
      "1037\n",
      "2017 5\n",
      "996\n",
      "2017 6\n",
      "1082\n",
      "2017 7\n",
      "1163\n",
      "2017 8\n",
      "1323\n",
      "2017 9\n",
      "1225\n",
      "2017 10\n",
      "1284\n",
      "2017 11\n",
      "1578\n",
      "2017 12\n",
      "1536\n",
      "2018 1\n",
      "1846\n",
      "2018 2\n",
      "1661\n",
      "2018 3\n",
      "1779\n",
      "2018 4\n",
      "1689\n",
      "2018 5\n",
      "1506\n",
      "2018 6\n",
      "1395\n",
      "2018 7\n",
      "1499\n",
      "2018 8\n",
      "1997\n",
      "2018 9\n",
      "2474\n",
      "2018 10\n",
      "2599\n",
      "2018 11\n",
      "2587\n",
      "2018 12\n",
      "2706\n",
      "2019 1\n",
      "3217\n",
      "2019 2\n",
      "2904\n",
      "2019 3\n",
      "3258\n",
      "2019 4\n",
      "2920\n",
      "2019 5\n",
      "3213\n",
      "2019 6\n",
      "3183\n",
      "2019 7\n",
      "3436\n",
      "2019 8\n",
      "3605\n",
      "2019 9\n",
      "3559\n",
      "2019 10\n",
      "3396\n",
      "2019 11\n",
      "3177\n",
      "2019 12\n",
      "3259\n",
      "2020 1\n",
      "4207\n",
      "2020 2\n",
      "3864\n",
      "2020 3\n",
      "4250\n",
      "2020 4\n",
      "2773\n"
     ]
    }
   ],
   "source": [
    "anxiety = crawl_reddit(2016,1,'Anxiety')\n",
    "anxiety.to_csv('anxiety_04_19_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 39423 entries, 0 to 1300\n",
      "Data columns (total 14 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   author                39423 non-null  object \n",
      " 1   created_utc           39423 non-null  int64  \n",
      " 2   id                    39423 non-null  object \n",
      " 3   link_flair_text       39418 non-null  object \n",
      " 4   num_comments          39423 non-null  int64  \n",
      " 5   score                 39423 non-null  int64  \n",
      " 6   selftext              39423 non-null  object \n",
      " 7   subreddit             39423 non-null  object \n",
      " 8   title                 39423 non-null  object \n",
      " 9   url                   39423 non-null  object \n",
      " 10  created               39423 non-null  object \n",
      " 11  d_                    39384 non-null  object \n",
      " 12  body_compound_score   39423 non-null  float64\n",
      " 13  title_compound_score  39423 non-null  float64\n",
      "dtypes: float64(2), int64(3), object(9)\n",
      "memory usage: 5.8+ MB\n"
     ]
    }
   ],
   "source": [
    "BPD_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>id</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>num_comments</th>\n",
       "      <th>score</th>\n",
       "      <th>selftext</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>created</th>\n",
       "      <th>d_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[deleted]</td>\n",
       "      <td>1454296789</td>\n",
       "      <td>43mkdn</td>\n",
       "      <td>Seeking Support</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>[deleted]</td>\n",
       "      <td>BPD</td>\n",
       "      <td>Fight with a coworker was publicly disrespectf...</td>\n",
       "      <td>https://www.reddit.com/r/BPD/comments/43mkdn/f...</td>\n",
       "      <td>1.45431e+09</td>\n",
       "      <td>{'author': '[deleted]', 'created_utc': 1454296...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SharpAtTheEdge</td>\n",
       "      <td>1454296727</td>\n",
       "      <td>43mk8p</td>\n",
       "      <td>Seeking Support</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>I just seem to fuck up every relationship I've...</td>\n",
       "      <td>BPD</td>\n",
       "      <td>I hate BPD so much.</td>\n",
       "      <td>https://www.reddit.com/r/BPD/comments/43mk8p/i...</td>\n",
       "      <td>1.45431e+09</td>\n",
       "      <td>{'author': 'SharpAtTheEdge', 'created_utc': 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>skyandbuildings</td>\n",
       "      <td>1454296592</td>\n",
       "      <td>43mjwl</td>\n",
       "      <td>Seeking Support</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>I think about killing myself all the time. As ...</td>\n",
       "      <td>BPD</td>\n",
       "      <td>I can't get the thought of suicide out of my h...</td>\n",
       "      <td>https://www.reddit.com/r/BPD/comments/43mjwl/i...</td>\n",
       "      <td>1.45431e+09</td>\n",
       "      <td>{'author': 'skyandbuildings', 'created_utc': 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The_JollyGreenGiant</td>\n",
       "      <td>1454296570</td>\n",
       "      <td>43mjul</td>\n",
       "      <td>Questions</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "      <td>Now I've realized that I've entered into a rel...</td>\n",
       "      <td>BPD</td>\n",
       "      <td>I thought I was fine over the past few weeks. ...</td>\n",
       "      <td>https://www.reddit.com/r/BPD/comments/43mjul/i...</td>\n",
       "      <td>1.45431e+09</td>\n",
       "      <td>{'author': 'The_JollyGreenGiant', 'created_utc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>justanotherikealamp</td>\n",
       "      <td>1454290090</td>\n",
       "      <td>43m3q5</td>\n",
       "      <td>Venting</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>I also have complex ptsd. I've been trapped in...</td>\n",
       "      <td>BPD</td>\n",
       "      <td>Is there anyone who could talk (or type) with ...</td>\n",
       "      <td>https://www.reddit.com/r/BPD/comments/43m3q5/i...</td>\n",
       "      <td>1.45431e+09</td>\n",
       "      <td>{'author': 'justanotherikealamp', 'created_utc...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                author  created_utc      id  link_flair_text  num_comments  \\\n",
       "0            [deleted]   1454296789  43mkdn  Seeking Support             2   \n",
       "1       SharpAtTheEdge   1454296727  43mk8p  Seeking Support             5   \n",
       "2      skyandbuildings   1454296592  43mjwl  Seeking Support             6   \n",
       "3  The_JollyGreenGiant   1454296570  43mjul        Questions            12   \n",
       "4  justanotherikealamp   1454290090  43m3q5          Venting             4   \n",
       "\n",
       "   score                                           selftext subreddit  \\\n",
       "0      1                                          [deleted]       BPD   \n",
       "1      4  I just seem to fuck up every relationship I've...       BPD   \n",
       "2      2  I think about killing myself all the time. As ...       BPD   \n",
       "3      5  Now I've realized that I've entered into a rel...       BPD   \n",
       "4     14  I also have complex ptsd. I've been trapped in...       BPD   \n",
       "\n",
       "                                               title  \\\n",
       "0  Fight with a coworker was publicly disrespectf...   \n",
       "1                                I hate BPD so much.   \n",
       "2  I can't get the thought of suicide out of my h...   \n",
       "3  I thought I was fine over the past few weeks. ...   \n",
       "4  Is there anyone who could talk (or type) with ...   \n",
       "\n",
       "                                                 url      created  \\\n",
       "0  https://www.reddit.com/r/BPD/comments/43mkdn/f...  1.45431e+09   \n",
       "1  https://www.reddit.com/r/BPD/comments/43mk8p/i...  1.45431e+09   \n",
       "2  https://www.reddit.com/r/BPD/comments/43mjwl/i...  1.45431e+09   \n",
       "3  https://www.reddit.com/r/BPD/comments/43mjul/i...  1.45431e+09   \n",
       "4  https://www.reddit.com/r/BPD/comments/43m3q5/i...  1.45431e+09   \n",
       "\n",
       "                                                  d_  \n",
       "0  {'author': '[deleted]', 'created_utc': 1454296...  \n",
       "1  {'author': 'SharpAtTheEdge', 'created_utc': 14...  \n",
       "2  {'author': 'skyandbuildings', 'created_utc': 1...  \n",
       "3  {'author': 'The_JollyGreenGiant', 'created_utc...  \n",
       "4  {'author': 'justanotherikealamp', 'created_utc...  "
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BPD_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning dataset notes\n",
    "- watch out for empty selftext\n",
    "- watch out for [deleted] in self text\n",
    "- author can also be [deleted] \n",
    "- Seems we cannot get ups and downs from this api, only final score "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General Subreddits for Advice or Venting\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advice subreddits (member count):\n",
    "- r/Advice (401k)\n",
    "- r/needadvice (381k) : limits discussions of certain topics, including relationships, but is flaired with question category (\"pet loss\", \"mental health\")\n",
    "\n",
    "Venting subreddits:\n",
    "- r/venting (6.1k)\n",
    "- r/Rant (170k)\n",
    "- r/Vent (40k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 1\n",
      "554\n",
      "2016 2\n",
      "490\n",
      "2016 3\n",
      "512\n",
      "2016 4\n",
      "512\n",
      "2016 5\n",
      "539\n",
      "2016 6\n",
      "520\n",
      "2016 7\n",
      "521\n",
      "2016 8\n",
      "594\n",
      "2016 9\n",
      "521\n",
      "2016 10\n",
      "538\n",
      "2016 11\n",
      "565\n",
      "2016 12\n",
      "611\n",
      "2017 1\n",
      "654\n",
      "2017 2\n",
      "530\n",
      "2017 3\n",
      "587\n",
      "2017 4\n",
      "445\n",
      "2017 5\n",
      "479\n",
      "2017 6\n",
      "511\n",
      "2017 7\n",
      "524\n",
      "2017 8\n",
      "513\n",
      "2017 9\n",
      "533\n",
      "2017 10\n",
      "564\n",
      "2017 11\n",
      "539\n",
      "2017 12\n",
      "575\n",
      "2018 1\n",
      "672\n",
      "2018 2\n",
      "703\n",
      "2018 3\n",
      "841\n",
      "2018 4\n",
      "835\n",
      "2018 5\n",
      "914\n",
      "2018 6\n",
      "909\n",
      "2018 7\n",
      "1102\n",
      "2018 8\n",
      "1072\n",
      "2018 9\n",
      "1130\n",
      "2018 10\n",
      "1191\n",
      "2018 11\n",
      "1126\n",
      "2018 12\n",
      "947\n",
      "2019 1\n",
      "1409\n",
      "2019 2\n",
      "1253\n",
      "2019 3\n",
      "1687\n",
      "2019 4\n",
      "1496\n",
      "2019 5\n",
      "1639\n",
      "2019 6\n",
      "1253\n",
      "2019 7\n",
      "1434\n",
      "2019 8\n",
      "1614\n",
      "2019 9\n",
      "1415\n",
      "2019 10\n",
      "1491\n",
      "2019 11\n",
      "1509\n",
      "2019 12\n",
      "1496\n",
      "2020 1\n",
      "1439\n",
      "2020 2\n",
      "1353\n",
      "2020 3\n",
      "1337\n",
      "2020 4\n",
      "546\n"
     ]
    }
   ],
   "source": [
    "needadvice = crawl_reddit(2016,1,'needadvice',filter_nonflaired=False)\n",
    "needadvice.head()\n",
    "needadvice.to_csv('needadvice_01_01_2016_04_19_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 1\n",
      "1155\n",
      "2016 2\n",
      "1214\n",
      "2016 3\n",
      "1398\n",
      "2016 4\n",
      "1331\n",
      "2016 5\n",
      "1531\n",
      "2016 6\n",
      "1570\n",
      "2016 7\n",
      "1664\n",
      "2016 8\n",
      "1801\n",
      "2016 9\n",
      "1756\n",
      "2016 10\n",
      "1751\n",
      "2016 11\n",
      "1938\n",
      "2016 12\n",
      "1805\n",
      "2017 1\n",
      "1884\n",
      "2017 2\n",
      "1656\n",
      "2017 3\n",
      "1965\n",
      "2017 4\n",
      "1786\n",
      "2017 5\n",
      "1884\n",
      "2017 6\n",
      "1949\n",
      "2017 7\n",
      "2037\n",
      "2017 8\n",
      "2300\n",
      "2017 9\n",
      "2236\n",
      "2017 10\n",
      "2333\n",
      "2017 11\n",
      "2252\n",
      "2017 12\n",
      "2441\n",
      "2018 1\n",
      "2380\n",
      "2018 2\n",
      "2562\n",
      "2018 3\n",
      "2918\n",
      "2018 4\n",
      "3023\n",
      "2018 5\n",
      "3238\n",
      "2018 6\n",
      "3032\n",
      "2018 7\n",
      "3536\n",
      "2018 8\n",
      "3508\n",
      "2018 9\n",
      "3642\n",
      "2018 10\n",
      "3880\n",
      "2018 11\n",
      "4198\n",
      "2018 12\n",
      "4423\n",
      "2019 1\n",
      "4813\n",
      "2019 2\n",
      "4724\n",
      "2019 3\n",
      "5027\n",
      "2019 4\n",
      "5269\n",
      "2019 5\n",
      "5248\n",
      "2019 6\n",
      "5264\n",
      "2019 7\n",
      "6287\n",
      "2019 8\n",
      "6522\n",
      "2019 9\n",
      "6275\n",
      "2019 10\n",
      "6574\n",
      "2019 11\n",
      "6762\n",
      "2019 12\n",
      "7414\n",
      "2020 1\n",
      "7594\n",
      "2020 2\n",
      "7333\n",
      "2020 3\n",
      "9206\n",
      "2020 4\n",
      "5355\n"
     ]
    }
   ],
   "source": [
    "rant = crawl_reddit(2016,1,'Rant',filter_nonflaired=False)\n",
    "rant.head()\n",
    "rant.to_csv('rant_01_01_2016_04_19_2020.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 1\n",
      "212\n",
      "2016 2\n",
      "186\n",
      "2016 3\n",
      "244\n",
      "2016 4\n",
      "263\n",
      "2016 5\n",
      "246\n",
      "2016 6\n",
      "236\n",
      "2016 7\n",
      "236\n",
      "2016 8\n",
      "246\n",
      "2016 9\n",
      "293\n",
      "2016 10\n",
      "301\n",
      "2016 11\n",
      "302\n",
      "2016 12\n",
      "340\n",
      "2017 1\n",
      "360\n",
      "2017 2\n",
      "391\n",
      "2017 3\n",
      "411\n",
      "2017 4\n",
      "399\n",
      "2017 5\n",
      "400\n",
      "2017 6\n",
      "430\n",
      "2017 7\n",
      "465\n",
      "2017 8\n",
      "590\n",
      "2017 9\n",
      "504\n",
      "2017 10\n",
      "576\n",
      "2017 11\n",
      "616\n",
      "2017 12\n",
      "746\n",
      "2018 1\n",
      "693\n",
      "2018 2\n",
      "769\n",
      "2018 3\n",
      "923\n",
      "2018 4\n",
      "996\n",
      "2018 5\n",
      "1069\n",
      "2018 6\n",
      "1110\n",
      "2018 7\n",
      "1237\n",
      "2018 8\n",
      "1391\n",
      "2018 9\n",
      "1385\n",
      "2018 10\n",
      "1684\n",
      "2018 11\n",
      "1827\n",
      "2018 12\n",
      "2036\n",
      "2019 1\n",
      "2300\n",
      "2019 2\n",
      "2455\n",
      "2019 3\n",
      "2816\n",
      "2019 4\n",
      "2757\n",
      "2019 5\n",
      "2900\n",
      "2019 6\n",
      "3205\n",
      "2019 7\n",
      "3473\n",
      "2019 8\n",
      "3624\n",
      "2019 9\n",
      "3585\n",
      "2019 10\n",
      "3780\n",
      "2019 11\n",
      "4133\n",
      "2019 12\n",
      "4762\n",
      "2020 1\n",
      "5055\n",
      "2020 2\n",
      "4906\n",
      "2020 3\n",
      "5901\n",
      "2020 4\n",
      "3628\n"
     ]
    }
   ],
   "source": [
    "vent = crawl_reddit(2016,1,'Vent',filter_nonflaired=False)\n",
    "vent.head()\n",
    "vent.to_csv('vent_01_01_2016_04_19_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 1\n",
      "5326\n",
      "2016 2\n",
      "5163\n",
      "2016 3\n",
      "5181\n",
      "2016 4\n",
      "5519\n",
      "2016 5\n",
      "6012\n",
      "2016 6\n",
      "6027\n",
      "2016 7\n",
      "6369\n",
      "2016 8\n",
      "7005\n",
      "2016 9\n",
      "6772\n",
      "2016 10\n",
      "6942\n",
      "2016 11\n",
      "6725\n",
      "2016 12\n",
      "7651\n",
      "2017 1\n",
      "8009\n",
      "2017 2\n",
      "7169\n",
      "2017 3\n",
      "7934\n",
      "2017 4\n",
      "7760\n",
      "2017 5\n",
      "8032\n",
      "2017 6\n",
      "8146\n",
      "2017 7\n",
      "8192\n",
      "2017 8\n",
      "8645\n",
      "2017 9\n",
      "8492\n",
      "2017 10\n",
      "9360\n",
      "2017 11\n",
      "9834\n",
      "2017 12\n",
      "9727\n",
      "2018 1\n",
      "10644\n",
      "2018 2\n",
      "9735\n",
      "2018 3\n",
      "10610\n",
      "2018 4\n",
      "11411\n",
      "2018 5\n",
      "12131\n",
      "2018 6\n",
      "11928\n",
      "2018 7\n",
      "13081\n",
      "2018 8\n",
      "14285\n",
      "2018 9\n",
      "14276\n",
      "2018 10\n",
      "16271\n",
      "2018 11\n",
      "16131\n",
      "2018 12\n",
      "17618\n",
      "2019 1\n",
      "19665\n",
      "2019 2\n",
      "19313\n",
      "2019 3\n",
      "22617\n",
      "2019 4\n",
      "22520\n",
      "2019 5\n",
      "23238\n",
      "2019 6\n",
      "24091\n",
      "2019 7\n",
      "26882\n",
      "2019 8\n",
      "27794\n",
      "2019 9\n",
      "26418\n",
      "2019 10\n",
      "26776\n",
      "2019 11\n",
      "26409\n",
      "2019 12\n",
      "29581\n",
      "2020 1\n",
      "30304\n",
      "2020 2\n",
      "28793\n",
      "2020 3\n",
      "30404\n",
      "2020 4\n",
      "18438\n"
     ]
    }
   ],
   "source": [
    "advice_df = crawl_reddit(2016,1,'Advice', filter_nonflaired=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "advice_df.to_csv('advice_01_01_2016_04_19_2020.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016 1\n",
      "58\n",
      "2016 2\n",
      "42\n",
      "2016 3\n",
      "43\n",
      "2016 4\n",
      "46\n",
      "2016 5\n",
      "46\n",
      "2016 6\n",
      "44\n",
      "2016 7\n",
      "54\n",
      "2016 8\n",
      "74\n",
      "2016 9\n",
      "72\n",
      "2016 10\n",
      "73\n",
      "2016 11\n",
      "49\n",
      "2016 12\n",
      "89\n",
      "2017 1\n",
      "70\n",
      "2017 2\n",
      "53\n",
      "2017 3\n",
      "85\n",
      "2017 4\n",
      "59\n",
      "2017 5\n",
      "107\n",
      "2017 6\n",
      "85\n",
      "2017 7\n",
      "80\n",
      "2017 8\n",
      "87\n",
      "2017 9\n",
      "67\n",
      "2017 10\n",
      "91\n",
      "2017 11\n",
      "86\n",
      "2017 12\n",
      "88\n",
      "2018 1\n",
      "113\n",
      "2018 2\n",
      "83\n",
      "2018 3\n",
      "95\n",
      "2018 4\n",
      "144\n",
      "2018 5\n",
      "111\n",
      "2018 6\n",
      "143\n",
      "2018 7\n",
      "146\n",
      "2018 8\n",
      "169\n",
      "2018 9\n",
      "167\n",
      "2018 10\n",
      "195\n",
      "2018 11\n",
      "185\n",
      "2018 12\n",
      "199\n",
      "2019 1\n",
      "220\n",
      "2019 2\n",
      "247\n",
      "2019 3\n",
      "274\n",
      "2019 4\n",
      "278\n",
      "2019 5\n",
      "278\n",
      "2019 6\n",
      "303\n",
      "2019 7\n",
      "381\n",
      "2019 8\n",
      "389\n",
      "2019 9\n",
      "395\n",
      "2019 10\n",
      "436\n",
      "2019 11\n",
      "472\n",
      "2019 12\n",
      "534\n",
      "2020 1\n",
      "536\n",
      "2020 2\n",
      "559\n",
      "2020 3\n",
      "626\n",
      "2020 4\n",
      "376\n"
     ]
    }
   ],
   "source": [
    "venting_df = crawl_reddit(2016,1,'venting',filter_nonflaired=False)\n",
    "venting_df.to_csv('venting_01_01_2016_04_19_2020.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Reddit API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Documentation for reference of data collection attempt directly with Python Reddit API.\n",
    "\n",
    "Problem with method: no way to retrieve historical data by specifying date range. Limit on number of results returned. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit= praw.Reddit(user_agent='Comment Extraction (by /u/USERNAME)',\n",
    "                    client_id='NEwAilxBlqBWeA', client_secret='***', \n",
    "                    username='***',password='***' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(reddit.user.me())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example on retrieving comment threads:\n",
    "\n",
    "https://github.com/akhilesh-reddy/Cable-cord-cutter-Sentiment-analysis-using-Reddit-data/blob/master/Scraping%20comments%20from%20Reddit%20forum.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seeking Support My ex got engaged after less than a year of dating someone else. Don’t know where else to talk about this. f1q0o6 1581339917.0 JordanLikeAStone 8 13 0 13\n",
      "[Comment(id='fh7okkx'), Comment(id='fh858ol'), Comment(id='fh8j9sd'), Comment(id='fh84wkl')]\n",
      "Urgent: Coping Skills Needed Should I go to the hospital for help with a meltdown??? f1uqeg 1581360256.0 carbeean 17 3 0 3\n",
      "[Comment(id='fh8iwwc'), Comment(id='fh8jcbw')]\n",
      "CW: Multiple DAE have a million thoughts and ideas constantly?? f1ia2k 1581295733.0 PeelMeDaddy 7 8 0 8\n",
      "[Comment(id='fh6ef5c'), Comment(id='fh6s911'), Comment(id='fh6imnm'), Comment(id='fh6vi40')]\n",
      "DAE DAE not have a full on fear of abandonment, but a fear of being hated? f1fu73 1581285314.0 nevillethrowaway22 6 10 0 10\n",
      "[Comment(id='fh57bwc'), Comment(id='fh578gx'), Comment(id='fh5npk5'), Comment(id='fh6x0y8')]\n",
      "Seeking Support Coping ahead for a Valentine's day alone f1i07c 1581294535.0 princessmudkip 6 3 0 3\n",
      "[Comment(id='fh6jc3b'), Comment(id='fh6qe20'), Comment(id='fh7f6fk')]\n",
      "Quiet Borderline Coronavirus & a BPD Perspective f1qnv5 1581343140.0 savantalicious 4 5 0 5\n",
      "[Comment(id='fh84ukj'), Comment(id='fh84v83'), Comment(id='fh8aedt'), Comment(id='fh84fo7')]\n",
      "Input What are people's thoughts on this? New therapist has never worked with BPD & gave me somewhat of a timeline on our program. f1shh7 1581351147.0 DaisyEyedOwl 5 3 0 3\n",
      "[Comment(id='fh8729x'), Comment(id='fh84aat'), Comment(id='fh84w5v'), Comment(id='fh88az8'), Comment(id='fh8bemr')]\n",
      "DAE DAE feels like they’re about to explode from happiness when they heard a certain song or smell something they like ? f1tx95 1581357033.0 zouillelamagouille 4 2 0 2\n",
      "[Comment(id='fh8hkiw'), Comment(id='fh8j50g')]\n",
      "DAE DAE struggle to stay in a job or study, when you find out someone is being rude to you? f1rqzx 1581348052.0 showshan98 2 9 0 9\n",
      "[Comment(id='fh81d2z'), Comment(id='fh83kix')]\n",
      "Progress Post I’ve seen a light I’d forgotten existed f1pmuu 1581337805.0 Peridot1969 3 3 0 3\n",
      "[Comment(id='fh7obe0'), Comment(id='fh7phzu'), Comment(id='fh7z7zg')]\n",
      "Urgent: Coping Skills Needed It's a day where I'm annoyed by everything and fucking hate everyone f1tw86 1581356908.0 OldSnacks 5 1 0 1\n",
      "[Comment(id='fh8ever')]\n",
      "Perspective Needed Can you have a healthy relationship with your FP? f1lrjx 1581312608.0 mekiiwe 3 1 0 1\n",
      "[Comment(id='fh7679n'), Comment(id='fh7fed6'), Comment(id='fh7g02q')]\n",
      "CW: Abuse I hid the fact I was previously diagnosed with BPD in a different country - How can I approach my GP now that I have major concerns? f1nch6 1581322412.0 ILoveSkulls 2 2 0 2\n",
      "[Comment(id='fh7bi2z'), Comment(id='fh7kqw7')]\n",
      "Fuck My Life Kicked out a BPD group on Facebook f1qusm 1581344011.0 Perversia_Rayne 2 1 0 1\n",
      "[Comment(id='fh7zk4h')]\n",
      "DAE Is this a BPD thing? f1ehy6 1581279967.0 raisingwildflowers 1 3 0 3\n",
      "[Comment(id='fh4rppd')]\n",
      "Lesson Learned I posted a while ago about my codependent relationship...and thank you for your help... f1krhu 1581307421.0 FlirtatiousEuph 0 3 0 3\n",
      "[]\n",
      "Seeking Support Just a gratuitous “I have BPD” post f1kfsl 1581305828.0 ScarlettLux 0 2 0 2\n",
      "[]\n",
      "Perspective Needed Favorite person ghosted me again a few weeks ago. f1hipz 1581292354.0 likeaprincess96 0 2 0 2\n",
      "[]\n",
      "Seeking Support Just lost a client and am feeling deep self-hatred f1vzq7 1581365200.0 Veryaburneraccount 0 1 0 1\n",
      "[]\n",
      "Medicine I have a feeling that my psychiatrist wants to put me on a higher dose/ change of mood stabilizers. Because I’ve been having lots of fluctuations, and extreme heightened sensitivity. But the good mood swings, which HAVE been the majority, have been SO AMAZING, and I don’t want them to go away!! f1v51a 1581361845.0 lookin_4Answers 0 1 0 1\n",
      "[]\n",
      "DAE Hey reddit, it's been a while. f1voot 1581364009.0 worriedsquishy 0 1 0 1\n",
      "[]\n",
      "Fuck My Life Today I sat on a toilet at school and cried f1s9ti 1581350271.0 emsy6 1 1 0 1\n",
      "[Comment(id='fh87dgi')]\n",
      "CW: Suicide So,I feel alone...and a burden. f1pfkf 1581336672.0 CheekyBunny1693 1 1 0 1\n",
      "[]\n",
      "Input Recently diagnosed and a break up f1sfcw 1581350904.0 sayhellogunnar 0 1 0 1\n",
      "[]\n",
      "CW: Substance Abuse I was feeling okay today. I was in a good mood. Then I talk to one person and that feeling is long gone. f1k3b1 1581304195.0 FrogginBullfish_ 0 1 0 1\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "#extracting comments from BPD subreddit, complete according to cordcutter example\n",
    "comm_list = []\n",
    "header_list = []\n",
    "i = 0\n",
    "for submission in reddit.subreddit('BPD').search('a',time_filter='week'):\n",
    "    print(submission.link_flair_text, submission.title, submission.id, submission.created_utc,\n",
    "          submission.author, submission.num_comments, submission.ups, submission.downs, submission.score\n",
    "         ) #submission.selftext, )\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    comment_queue = submission.comments[:]\n",
    "    print(comment_queue)\n",
    "\n",
    "\n",
    "## cordcutter example    \n",
    "comm_list = []\n",
    "header_list = []\n",
    "i = 0\n",
    "for submission in reddit.subreddit('cordcutters').hot(limit=2):\n",
    "    submission.comments.replace_more(limit=None)\n",
    "    comment_queue = submission.comments[:]  # Seed with top-level\n",
    "    while comment_queue:\n",
    "        header_list.append(submission.title)\n",
    "        comment = comment_queue.pop(0)\n",
    "        comm_list.append(comment.body)\n",
    "        t = []\n",
    "        t.extend(comment.replies)\n",
    "        while t:\n",
    "            header_list.append(submission.title)\n",
    "            reply = t.pop(0)\n",
    "            comm_list.append(reply.body)\n",
    "df = pd.DataFrame(header_list)\n",
    "df['comm_list'] = comm_list\n",
    "df.columns = ['header','comments']\n",
    "df['comments'] = df['comments'].apply(lambda x : x.replace('\\n',''))\n",
    "df.to_csv('cordcutter_comments.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list of lists for submissions objects\n",
    "submission_list = []\n",
    "\n",
    "for submission in reddit.subreddit('BPD').search('a',time_filter='all', sort='new', limit=1000):\n",
    "    submission_list.append([submission.id, submission.subreddit, submission.created_utc, submission.link_flair_text, \n",
    "                            submission.author, submission.title, submission.selftext, submission.num_comments, \n",
    "                            submission.ups, submission.downs, submission.score])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "submissions_df = pd.DataFrame(submission_list, columns=['id','subreddit','created_utc', 'link_flair_text', 'author', 'title',\n",
    "                                      'body', 'num_comments', 'ups', 'downs', 'score'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
